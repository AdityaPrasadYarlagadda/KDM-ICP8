{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDM_icp8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciwdMQC0J5Uc",
        "outputId": "a7440ccd-1f56-48e8-dbed-39d1deac04b9"
      },
      "source": [
        "!pip install textacy\r\n",
        "import spacy\r\n",
        "import textacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/99/054efc5dea92c84a850639c490541de6cba29bc148debc3c73848c5e64c2/textacy-0.10.1-py3-none-any.whl (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 8.3MB/s \n",
            "\u001b[?25hCollecting jellyfish>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/a6/4d039bc827a102f62ce7a7910713e38fdfd7c7a40aa39c72fb14938a1473/jellyfish-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.4.1)\n",
            "Requirement already satisfied: cachetools>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.41.1)\n",
            "Collecting cytoolz>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/67/1c60da8ba831bfefedb64c78b9f6820bdf58972797c95644ee3191daf27a/cytoolz-0.11.0.tar.gz (477kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.5)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn<0.24.0,>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.22.2.post1)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.2.4)\n",
            "Requirement already satisfied: pyemd>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.5.1)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.0.1)\n",
            "Requirement already satisfied: srsly>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.0.5)\n",
            "Collecting pyphen>=0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 13.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz>=0.8.0->textacy) (0.11.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->textacy) (4.4.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (54.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (3.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.2.0->textacy) (3.4.1)\n",
            "Building wheels for collected packages: cytoolz\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.11.0-cp37-cp37m-linux_x86_64.whl size=1223227 sha256=008d2786189170cfd174fa02914a3e08d5f73cb21a91673a46cf5fe867419ccb\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/32/3c/9c9926b510647cacdde744b2c7acdf1ccd5896fbb7f8d5df0c\n",
            "Successfully built cytoolz\n",
            "Installing collected packages: jellyfish, cytoolz, pyphen, textacy\n",
            "Successfully installed cytoolz-0.11.0 jellyfish-0.8.2 pyphen-0.10.0 textacy-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaPAbAeFK66m",
        "outputId": "9014cd88-ac42-46da-deed-d9fe6dde9fb7"
      },
      "source": [
        "# reading input files\r\n",
        "file = open('/content/article.txt', 'r')\r\n",
        "# loading en_core_web_sm model\r\n",
        "nlp = spacy.load(\"en_core_web_sm\")\r\n",
        "doc = nlp(file.read())\r\n",
        "print(doc)\r\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you have gathered gigabytes or terabytes of unstructured text, for instance scraping the Internet, or pieces of email from your employees or users, or tweets, or millions of products that you want to categorize (only product description and product name is available - sometimes with typos). Now you want to make sense of it, and extract value, possibly design a nice search engine so that your customers can easily find your products. The core algorithm that you need is an automated cataloguer, also called indexer. I am going to explain in layman's terms how it works.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuZ_c9JrLzjf",
        "outputId": "6bcd73b1-5573-4841-d609-48b2716af7bd"
      },
      "source": [
        "for token in doc:\r\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you PRON nsubj\n",
            "have AUX aux\n",
            "gathered VERB ROOT\n",
            "gigabytes NOUN dobj\n",
            "or CCONJ cc\n",
            "terabytes NOUN conj\n",
            "of ADP prep\n",
            "unstructured ADJ amod\n",
            "text NOUN pobj\n",
            ", PUNCT punct\n",
            "for ADP prep\n",
            "instance NOUN pobj\n",
            "scraping VERB acl\n",
            "the DET det\n",
            "Internet NOUN dobj\n",
            ", PUNCT punct\n",
            "or CCONJ cc\n",
            "pieces NOUN nsubj\n",
            "of ADP prep\n",
            "email NOUN pobj\n",
            "from ADP prep\n",
            "your DET poss\n",
            "employees NOUN pobj\n",
            "or CCONJ cc\n",
            "users NOUN conj\n",
            ", PUNCT punct\n",
            "or CCONJ cc\n",
            "tweets NOUN conj\n",
            ", PUNCT punct\n",
            "or CCONJ cc\n",
            "millions NOUN conj\n",
            "of ADP prep\n",
            "products NOUN pobj\n",
            "that DET dobj\n",
            "you PRON nsubj\n",
            "want VERB relcl\n",
            "to PART aux\n",
            "categorize VERB xcomp\n",
            "( PUNCT punct\n",
            "only ADJ advmod\n",
            "product NOUN compound\n",
            "description NOUN dobj\n",
            "and CCONJ cc\n",
            "product NOUN compound\n",
            "name NOUN conj\n",
            "is AUX conj\n",
            "available ADJ acomp\n",
            "- PUNCT punct\n",
            "sometimes ADV advmod\n",
            "with ADP prep\n",
            "typos PROPN pobj\n",
            ") PUNCT punct\n",
            ". PUNCT punct\n",
            "Now ADV advmod\n",
            "you PRON nsubj\n",
            "want VERB ROOT\n",
            "to PART aux\n",
            "make VERB xcomp\n",
            "sense NOUN dobj\n",
            "of ADP prep\n",
            "it PRON pobj\n",
            ", PUNCT punct\n",
            "and CCONJ cc\n",
            "extract NOUN compound\n",
            "value NOUN nsubj\n",
            ", PUNCT punct\n",
            "possibly ADV advmod\n",
            "design VERB conj\n",
            "a DET det\n",
            "nice ADJ amod\n",
            "search NOUN compound\n",
            "engine NOUN dobj\n",
            "so SCONJ mark\n",
            "that SCONJ mark\n",
            "your DET poss\n",
            "customers NOUN nsubj\n",
            "can VERB aux\n",
            "easily ADV advmod\n",
            "find VERB advcl\n",
            "your DET poss\n",
            "products NOUN dobj\n",
            ". PUNCT punct\n",
            "The DET det\n",
            "core NOUN compound\n",
            "algorithm NOUN nsubj\n",
            "that DET dobj\n",
            "you PRON nsubj\n",
            "need VERB relcl\n",
            "is AUX ccomp\n",
            "an DET det\n",
            "automated VERB amod\n",
            "cataloguer NOUN attr\n",
            ", PUNCT punct\n",
            "also ADV advmod\n",
            "called VERB ROOT\n",
            "indexer NOUN dobj\n",
            ". PUNCT punct\n",
            "I PRON nsubj\n",
            "am AUX aux\n",
            "going VERB ROOT\n",
            "to PART aux\n",
            "explain VERB xcomp\n",
            "in ADP prep\n",
            "layman PROPN poss\n",
            "'s PART case\n",
            "terms NOUN pobj\n",
            "how ADV advmod\n",
            "it PRON nsubj\n",
            "works VERB ccomp\n",
            ". PUNCT punct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr1vw3YXL_9r"
      },
      "source": [
        "tuples_list = []\r\n",
        "#extracting triplet using textacy library\r\n",
        "tuples = textacy.extract.subject_verb_object_triples(doc)\r\n",
        "tuples_to_list = list(tuples)\r\n",
        "if tuples_to_list != []:\r\n",
        "    tuples_list.append(tuples_to_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D9xMpPcMGaO",
        "outputId": "40dc0e36-7822-4097-a93b-ab3abbd195f4"
      },
      "source": [
        "print(tuples_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(you, have gathered, gigabytes), (you, have gathered, terabytes), (you, want, to categorize), (you, want, to make), (extract value, design, search engine), (customers, find, products), (I, am going, to explain)]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}